{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Profile Classification Model\n",
    "## Standard Classification workflow demonstration\n",
    "\n",
    "We're going to classify Argo data with the following workflow:\n",
    " - Load netcdf Argo profiles on Standard Depth Levels\n",
    " - Normalize profiles\n",
    " - Compress the collection with PCA (dimensionality reduction)\n",
    " - Train a GMM on the reduced dataset\n",
    " - Classify all the dataset\n",
    " - Compute class profile statistics\n",
    " - Create a map with labels by profiles\n",
    "\n",
    "Along the way, we'll produce figures to be compared with Maze et al, POC, 2017\n",
    "\n",
    "@author: gmaze@ifremer.fr\n",
    "\n",
    "More documentations:\n",
    "- http://scikit-learn.org\n",
    "- http://xarray.pydata.org\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "- http://scikit-learn.org/stable/modules/mixture.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Import modules and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*First, let's make sure the Python env is correct to run this notebook*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/psmkfjds7xsc4kjsz66ghldr000nsn/T/tmpcm9a2hhe/utils.py:66: UserWarning: \n",
      "Running on your own environment\n",
      "Make sure to have all necessary packages installed\n",
      "See: https://github.com/obidam/ds2-2023/blob/main/binder/environment.yml\n",
      "  warnings.warn(\"\\nRunning on your own environment\\nMake sure to have all necessary packages installed\\nSee: https://github.com/obidam/ds2-2023/blob/main/binder/environment.yml\")\n"
     ]
    }
   ],
   "source": [
    "import os, sys, urllib, tempfile\n",
    "with tempfile.TemporaryDirectory() as local:\n",
    "    sys.path.append(local)\n",
    "    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/obidam/ds2-2024/main/utils.py\", os.path.join(local, \"utils.py\"))\n",
    "    from utils import check_up_env\n",
    "    check_up_env(with_tuto=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.8.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dateutil\n",
    "dateutil.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Then, import the usual suspects:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray, dask\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "plt.rcParams['figure.figsize'] = (15,10)\n",
    "%matplotlib inline\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/mixture.html\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "import seaborn as sns\n",
    "from intake import open_catalog\n",
    "\n",
    "from tuto_tools import vrange, vrangec, plot_GMMellipse, sns_GMMellipse, create_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Define key parameters of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Number of GMM classes to compute:\n",
    "K = 8\n",
    "\n",
    "# Compression level for the dimensionality reduction \n",
    "maxvar = 99.9 # in %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load data and create a X[Np,Nz] array to work with\n",
    "\n",
    "From Google cloud storage [see specific tuto here](https://github.com/obidam/ds2-2022/blob/main/practice/environment/02-Access_to_data_in_the_cloud.ipynb).\n",
    "\n",
    "Use in-situ measurements from Argo floats, interpolated on standard depth levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_url = 'https://raw.githubusercontent.com/obidam/ds2-2024/main/ds2_data_catalog.yml'\n",
    "cat = open_catalog(catalog_url)\n",
    "ds = cat['argo_global_sdl_homogeneous'].read_chunked()\n",
    "\n",
    "# Squeeze data to the North Atlantic domain:\n",
    "ds = ds.where(ds['LATITUDE'].compute()>0, drop=True)\n",
    "ds = ds.where(ds['LONGITUDE'].compute()>=-80, drop=True)\n",
    "ds = ds.where(ds['LONGITUDE'].compute()<=0, drop=True)\n",
    "\n",
    "#\n",
    "print('This dataset holds: %.3f MB' % (ds.nbytes / 1e6))\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select the depth layer for training:\n",
    "#dsub = ds.sel(DEPTH=slice(-100,-1000))\n",
    "dsub = ds.sel(DEPTH=slice(0,-1400))\n",
    "\n",
    "# And Create the array X(Nz,Np): The field to classify with a GMM, \n",
    "# Np profiles with Nz depth levels.\n",
    "X = dsub['TEMP'].values\n",
    "DPTmodel = dsub['DEPTH'].values\n",
    "lon = dsub['LONGITUDE'].values\n",
    "lat = dsub['LATITUDE'].values\n",
    "          \n",
    "# Size of the training set X:\n",
    "[Np, Nz] = X.shape\n",
    "print(\"Number of raw features (Depth Levels): \", Nz)\n",
    "print(\"Number of samples (N profiles): \", Np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Normalization\n",
    "We operate along feature dimensions to:\n",
    "- Remove the sample mean\n",
    "- Divide by the sample std\n",
    "\n",
    "one depth level at a time\n",
    "\n",
    "Create the Xn[Np,Nz] array\n",
    "\n",
    "Doc:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Fit the scaler object:\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler = scaler.fit(X)\n",
    "\n",
    "# The mean and std profiles are in the scaler object properties:\n",
    "X_ave = scaler.mean_\n",
    "X_std = scaler.scale_\n",
    "\n",
    "# Normalize data:\n",
    "Xn = scaler.transform(X)       \n",
    "\n",
    "# Here, we only center data:\n",
    "Xc = preprocessing.StandardScaler(with_std=False).fit(X).transform(X)\n",
    "\n",
    "# Compute additional statistics, like the observed PDFz:\n",
    "def diag_pdfz(X,xedges):\n",
    "    Nz = X.shape[1]\n",
    "    PDFz = np.zeros((xedges.shape[0]-1,Nz))\n",
    "    for iz in np.arange(Nz):\n",
    "        h, hx = np.histogram(X[:,iz],bins=xedges,density=True)\n",
    "        PDFz[:,iz] = h\n",
    "    PDFz_axis = hx[0:-1]\n",
    "    return PDFz, PDFz_axis\n",
    "PDFz, PDFz_axis = diag_pdfz(X,np.arange(0,30,0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey='row', figsize=(10,5), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax[0].plot(X_ave, DPTmodel, '-', linewidth=2,label='Sample Mean')\n",
    "ax[1].plot(X_std, DPTmodel, '-', linewidth=2,label='Sample Std')\n",
    "# tidy up the figure\n",
    "ax[0].set_ylabel('Model Depth')\n",
    "for ix in range(0,2):\n",
    "    ax[ix].legend(loc='lower right')\n",
    "    ax[ix].grid(True)\n",
    "    ax[ix].set_xlabel('[degC]')\n",
    "fig.suptitle('Training Set Standardization profiles', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Select 100 random profiles:\n",
    "n = 150\n",
    "ip = np.unique(np.random.randint(0,Np-1,n))\n",
    "\n",
    "# Random selection of profiles\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharey='row', figsize=(10,5), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax[0].plot(X[ip,:].T, np.reshape(np.repeat(DPTmodel,ip.shape[0]),[Nz,ip.shape[0]]), '-', color='gray', linewidth=1)\n",
    "ax[0].plot(np.mean(X[ip,:].T,axis=1), DPTmodel, '-', color='k', linewidth=2)\n",
    "ax[0].plot(np.mean(X[ip,:].T,axis=1)-np.std(X[ip,:].T,axis=1), DPTmodel, '--', color='k', linewidth=2)\n",
    "ax[0].plot(np.mean(X[ip,:].T,axis=1)+np.std(X[ip,:].T,axis=1), DPTmodel, '--', color='k', linewidth=2)\n",
    "ax[0].grid(True)\n",
    "ax[0].set_title('Temperature profiles (random selection)')\n",
    "ax[0].set_xlabel('[degC]')\n",
    "\n",
    "cmap = plt.get_cmap('Paired',lut=128)\n",
    "df = xarray.DataArray(PDFz.T, coords=[DPTmodel,PDFz_axis], dims=['Depth','[degC]'])\n",
    "p = df.plot(cmap=cmap,vmin=0,vmax=0.6,ax=ax[1])\n",
    "ax[1].set_title(\"Observed PDFz\")\n",
    "\n",
    "fig.suptitle('Training Set profiles', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,5), dpi=80, facecolor='w', edgecolor='k')\n",
    "sns.set(context=\"notebook\",style=\"whitegrid\", palette=\"deep\", color_codes=True)\n",
    "iz = 30\n",
    "sns.histplot(X[:,iz], color=\"m\", kde=True, ax=ax, stat='density')\n",
    "ax.set_xlabel(\"PDF at %0.2fm depth\"%(DPTmodel[iz]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "iz1 = 1\n",
    "iz2 = np.argmax(DPTmodel<=-300)\n",
    "x = X[:,iz1]\n",
    "y = X[:,iz2]\n",
    "#fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,5), dpi=80, facecolor='w', edgecolor='k')\n",
    "sns.set(context=\"notebook\", style=\"whitegrid\", color_codes=True)\n",
    "sns.jointplot(x=x, y=y, kind=\"kde\",\\\n",
    "              xlim=vrange([x,y]),ylim=vrange([x,y]),\\\n",
    "              height=7, ratio=4, space=0)\\\n",
    "            .set_axis_labels(\"PDF at %0.2fm depth\"%(DPTmodel[iz1]), \"PDF at %0.2fm depth\"%(DPTmodel[iz2]))\n",
    "plt.suptitle(\"%0.2fm vs %0.2fm depth\"%(DPTmodel[iz1],DPTmodel[iz2]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Reduction\n",
    "Now that we have a standardized collection of profiles, let's compress it with a PCA decomposition\n",
    "\n",
    "The goal here, is to reduce the dimensionality of the problem from N depths levels down to a couple of principal components\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\t\\mathbf{x}(z) &=& \\sum_{j=1}^{Nc} \\mathbf{P}(z,j) \\mathbf{y}(j)\n",
    "\\end{eqnarray}\n",
    "where $\\mathbf{P}\\in \\mathbb{R}^{Nz\\times Nc}$ and $\\mathbf{y}\\in \\mathbb{R}^{Nc\\times Np}$ with $Nc\\leq Nz$. \n",
    "The first rows of $\\mathbf{P}$ contain profiles maximizing the structural variance throughout the collection of profiles.\n",
    "\n",
    "Create the Xr[Np,Nc] array\n",
    "\n",
    "Doc: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Compute P (the EOFs) from x:\n",
    "reducer = PCA(n_components=maxvar/100,svd_solver='full')\n",
    "reducer.fit(Xn)\n",
    "\n",
    "# Reduce the dataset (compute the y):\n",
    "Xr = reducer.transform(Xn) # Here we compute: np.dot(Xn - reducer.mean_, np.transpose(reducer.components_))\n",
    "\n",
    "# Variables of the reduced space:\n",
    "Nc = reducer.n_components_ # Number of components retained\n",
    "EOFs = reducer.components_ # [Nc , Nz], the P matrix\n",
    "V = reducer.explained_variance_ratio_ # Explained variance, with 0 to 1 values\n",
    "\n",
    "# We can also compute EOFs with real units this way:\n",
    "S = np.sqrt(reducer.explained_variance_*Xn.shape[0]) # These are the singular values\n",
    "Z = np.dot(Xn - reducer.mean_, np.transpose(reducer.components_)) # This is simply Xr or the principal components\n",
    "Ztilde = Z/np.sqrt(S) # Normalized PCs\n",
    "#EOFs_real = np.dot(np.transpose(Ztilde),X)/X.shape[0] # Regression on any collection of profiles\n",
    "EOFs_realc = np.dot(np.transpose(Ztilde),Xc)/Xc.shape[0] # Regression on any collection of profiles\n",
    "EOFs_real = np.dot(np.transpose(Ztilde),Xn)/Xn.shape[0] # Regression on any collection of profiles\n",
    "\n",
    "# Compute the RMS difference between the reconstructed and original dataset:\n",
    "Xn_reconstructed = reducer.inverse_transform(Xr)\n",
    "X_reconstructed = scaler.inverse_transform(Xn_reconstructed)\n",
    "rms = np.sqrt(np.mean(np.square(X_reconstructed-X),axis=0))\n",
    "\n",
    "#\n",
    "print(\"\\nWe reduced the dimensionality of the problem from %i depth levels down to %i PCs\\n\"%(Nz,Nc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Figures with PCA decomposition details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,5), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax.bar(range(0,Nc),V*100)\n",
    "ax.set_xlabel('i PCA component')\n",
    "ax.set_ylabel('% of variance explained')\n",
    "ax.grid(True)\n",
    "ax.set_xticks(range(0,Nc))\n",
    "ax.set_title('Variance explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20,8), dpi=160, facecolor='w', edgecolor='k', sharey='row')\n",
    "\n",
    "iax = 0\n",
    "xl = np.max(np.abs([np.min(EOFs),np.max(EOFs)]))\n",
    "for ie in range(0,4):\n",
    "    ax[iax].plot(np.transpose(EOFs[ie,:]),DPTmodel,label=\"PCA-%i\"%(ie+1))\n",
    "ax[iax].set_xlim(1.1*np.array([-xl,xl]))\n",
    "ax[iax].legend(loc='lower right')\n",
    "ax[iax].set_xlabel('PCA components (no units)')\n",
    "ax[iax].set_ylabel('Model Depth')\n",
    "ax[iax].grid(True)\n",
    "ax[iax].set_title('PCA 1-4 (no units)')\n",
    "\n",
    "iax+=1\n",
    "xl = np.max(np.abs([np.min(EOFs_real),np.max(EOFs_real)]))\n",
    "for ie in range(0,4):\n",
    "    ax[iax].plot(np.transpose(EOFs_real[ie,:]),DPTmodel,label=\"PCA-%i\"%(ie+1))\n",
    "ax[iax].set_xlim(1.1*np.array([-xl,xl]))\n",
    "ax[iax].legend(loc='lower left')\n",
    "ax[iax].set_xlabel('PCA components (normalized units)')\n",
    "ax[iax].set_ylabel('Model Depth')\n",
    "ax[iax].grid(True)\n",
    "ax[iax].set_title('PCA 1-4 (real units)')\n",
    "\n",
    "iax+=1\n",
    "xl = np.max(np.abs([np.min(EOFs_realc),np.max(EOFs_realc)]))\n",
    "for ie in range(0,4):\n",
    "    ax[iax].plot(np.transpose(EOFs_realc[ie,:]),DPTmodel,label=\"PCA-%i\"%(ie+1))\n",
    "ax[iax].set_xlim(1.1*np.array([-xl,xl]))\n",
    "ax[iax].legend(loc='lower left')\n",
    "ax[iax].set_xlabel('PCA components (centered units)')\n",
    "ax[iax].set_ylabel('Model Depth')\n",
    "ax[iax].grid(True)\n",
    "ax[iax].set_title('PCA 1-4 (real units)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,5), dpi=80, facecolor='w', edgecolor='k')\n",
    "ax.semilogx(rms, DPTmodel, '-', linewidth=2,label=\"d=%i (%0.2f %%)\"%(Nc,np.sum(V*100)))\n",
    "# tidy up the figure\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_xticks([0.0125,0.025,0.05,0.1,0.25,0.5,1])\n",
    "ax.set_xlim([1e-3,1e1])\n",
    "ax.set_xlabel('[real unit]')\n",
    "ax.set_ylabel('Model Depth')\n",
    "ax.grid(True)\n",
    "fig.suptitle('RMS difference between reduced and original dataset', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## GMM Classification\n",
    "We classify with a GMM the reduce dataset\n",
    "\n",
    "Doc: http://scikit-learn.org/stable/modules/mixture.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set-up and train the classifier:\n",
    "gmm = GaussianMixture(n_components=K,\\\n",
    "                      covariance_type='full',\\\n",
    "                      init_params='kmeans',\\\n",
    "                      max_iter=1000,\\\n",
    "                      tol=1e-6)\n",
    "gmm.fit(Xr) # Training on reduced data\n",
    "\n",
    "# Extract GMM parameters:\n",
    "priors = gmm.weights_ # [K,1]\n",
    "centers= gmm.means_   # [K,Nc]\n",
    "covars = gmm.covariances_ # [K,Nc,Nc] if 'full'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": true,
    "editable": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Classify the dataset:\n",
    "LABELS = gmm.predict(Xr) # [Np,1]\n",
    "POST   = gmm.predict_proba(Xr) # [Np,Nc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Time for a lot of figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(8,8), dpi=80, facecolor='w', edgecolor='k')\n",
    "id = np.array([1,2])-1\n",
    "ax.scatter(Xr[:,id[0]],Xr[:,id[1]],1)\n",
    "ax.grid(True)\n",
    "ax.axis('equal')\n",
    "ax.set_xlabel(\"Dimension #%i\"%(id[0]+1))\n",
    "ax.set_ylabel(\"Dimension #%i\"%(id[1]+1))\n",
    "colors = mpl.colors.cnames.items()\n",
    "colors = iter(plt.cm.rainbow(np.linspace(0, 1, K)))\n",
    "for ik in np.arange(K):\n",
    "    el,ax = plot_GMMellipse(gmm, id, ik, next(colors), ax, label=\"Class-%i\"%(ik+1), plot_kw={'linewidth':4})\n",
    "#     el,ax = plot_GMMellipse(gmm, id, ik, next(colors), ax, label=\"Class-%i\"%(ik+1))\n",
    "#    el,ax = plot_GMMellipse(gmm,id,ik,next(colors),ax,label=\"Class-%i\"%(ik+1),std=[1],linewidth=4)\n",
    "ax.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(Xr[:,id], columns=[\"x\", \"y\"])\n",
    "sns.set(context=\"notebook\",style=\"whitegrid\", color_codes=True)\n",
    "colors = iter(plt.cm.rainbow(np.linspace(0, 1, K)))\n",
    "\n",
    "g = sns.JointGrid(x=\"x\", y=\"y\", data=df, height=7, ratio=4, space=0.1,\n",
    "                  xlim=vrangec(Xr[:,id]),ylim=vrangec(Xr[:,id]))\n",
    "\n",
    "g.plot_marginals(sns.histplot, kde=False, stat='density', color=\".5\")\n",
    "\n",
    "g.plot_joint(plt.scatter, c=\".5\", s=1, linewidth=1, marker=\"+\")\n",
    "\n",
    "g.plot_joint(sns_GMMellipse, gmm=gmm, id=id, main_axes=False, colors=colors, linewidth=3)\n",
    "\n",
    "g.set_axis_labels(\"Dimension #%i\"%(id[0]+1), \"Dimension #%i\"%(id[1]+1))\n",
    "g.fig.suptitle(\"PC-%i vs PC-%i\"%(id[1]+1,id[0]+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ii in np.arange(2,4):\n",
    "    id = np.array([1,ii])-1\n",
    "    df = pd.DataFrame(Xr[:,id], columns=[\"x\", \"y\"])\n",
    "    sns.set(context=\"notebook\", style=\"whitegrid\", color_codes=True)\n",
    "    colors = iter(plt.cm.rainbow(np.linspace(0, 1, K)))\n",
    "    \n",
    "    g = sns.JointGrid(x=\"x\", y=\"y\", data=df, height=7, ratio=4, space=0.1,\n",
    "                      xlim=vrangec(Xr[:,id]),ylim=vrangec(Xr[:,id]))\n",
    "    g.plot_joint(sns.kdeplot, fill=True, \\\n",
    "                 thresh=0.05,\\\n",
    "                 cmap=sns.light_palette(\"gray\",reverse=False,as_cmap=True), n_levels=30)\n",
    "\n",
    "    g.plot_marginals(sns.kdeplot, color=\"k\", fill=True)\n",
    "\n",
    "    g.plot_joint(sns_GMMellipse, gmm=gmm, id=id, main_axes=False, colors=colors, linewidth=3)\n",
    "\n",
    "    g.set_axis_labels(\"Dimension #%i\"%(id[0]+1), \"Dimension #%i\"%(id[1]+1))\n",
    "    g.fig.suptitle(\"PC-%i vs PC-%i\"%(id[1]+1,id[0]+1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plot LABELS map:\n",
    "unique_labels = set(LABELS)\n",
    "colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "\n",
    "fig, proj, ax = create_map(extent=[-90, 0, -5, 70], dpi=80)\n",
    "\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 0.2]\n",
    "    class_member_mask = (LABELS == k)\n",
    "\n",
    "    plt.plot(lon[class_member_mask],\n",
    "                lat[class_member_mask],\n",
    "                 'o', markerfacecolor=tuple(col),\n",
    "                 markeredgecolor='none', markersize=2,  \n",
    "             label=\"Class %i\"%(k), transform=proj)\n",
    "\n",
    "plt.title(\"LABELS (K=%i)\"%(K))\n",
    "# plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Plot POSTERIORS map:\n",
    "for k, col in zip(unique_labels, colors):\n",
    "    fig, proj, ax = create_map(extent=[-90, 0, -5, 70], dpi=60)\n",
    "    if k == -1:\n",
    "        # Black used for noise.\n",
    "        col = [0, 0, 0, 0.2]\n",
    "#     class_member_mask = (LABELS == k)\n",
    "\n",
    "    plt.scatter(lon,lat,1,POST[:,k], transform=proj)\n",
    "\n",
    "    plt.title(\"POST (K=%i)\"%(k+1),fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Compute and plot the robustness of the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2",
   "language": "python",
   "name": "ds2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
