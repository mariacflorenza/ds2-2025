{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Clustering - Tuto 1\n",
    "\n",
    "In this tuto we play with a realistic 1D dataset: Vertical mean temperature from Argo data\n",
    "\n",
    "We use clustering with GMM to analyse the PDF of the dataset.\n",
    "\n",
    "The goal is to understand better how GMM works, and its limitations.\n",
    "\n",
    "The optimal number of clusters is determined with BIC and independant samples issues are demonstrated\n",
    "\n",
    "(c) G. Maze\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "*First, let's make sure the Python env is correct to run this notebook*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/psmkfjds7xsc4kjsz66ghldr000nsn/T/tmpphhnez2i/utils.py:44: UserWarning: \n",
      "Running on your own environment\n",
      "  warnings.warn(\"\\nRunning on your own environment\")\n"
     ]
    }
   ],
   "source": [
    "import os, sys, urllib, tempfile\n",
    "with tempfile.TemporaryDirectory() as local:\n",
    "    sys.path.append(local)\n",
    "    urllib.request.urlretrieve(\"https://raw.githubusercontent.com/obidam/ds2-2024/main/utils.py\", os.path.join(local, \"utils.py\"))\n",
    "    from utils import check_up_env\n",
    "    check_up_env(with_tuto=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'intake'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 8>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m stats\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mscipy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m signal\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mintake\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m open_catalog\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m preprocessing\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'intake'"
     ]
    }
   ],
   "source": [
    "# Libraries import section\n",
    "import os, sys\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy import signal\n",
    "from intake import open_catalog\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "import matplotlib\n",
    "# matplotlib.use('agg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.cm as cm\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import seaborn as sns\n",
    "sns.set(context=\"notebook\", style=\"whitegrid\", palette=\"deep\", color_codes=True)\n",
    "\n",
    "from tuto_tools import create_map, gaussian, plot_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "From Google cloud storage [see specific tuto here](https://github.com/obidam/ds2-2023/blob/main/practice/environment/02-Access_to_data_in_the_cloud.ipynb).\n",
    "\n",
    "We work with a 1-dimensional dataset: local 0-2000m vertical mean measurements (eg: temperature, salinity), from Argo floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_url = 'https://raw.githubusercontent.com/obidam/ds2-2024/main/ds2_data_catalog.yml'\n",
    "cat = open_catalog(catalog_url)\n",
    "ds = cat['argo_global_vertical_mean'].read_chunked()\n",
    "print('This dataset holds: %.3f GB' % (ds.nbytes / 1e9))\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats for 1D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Xlabel = ds['TEMP'], \"VERTICAL MEAN TEMP\"\n",
    "# X = X[np.where(X>3)]\n",
    "print(X)\n",
    "print(Xlabel)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, proj, ax = create_map()\n",
    "plt.scatter(X.LONGITUDE, X.LATITUDE, 1, X, cmap=plt.cm.get_cmap('gist_ncar',8), vmin=0, vmax=16)\n",
    "plt.colorbar()\n",
    "plt.title(Xlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=plt.figaspect(0.5), dpi=120)\n",
    "axes.set_title('Histogram')\n",
    "sns.histplot(X, ax=axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data\n",
    "\n",
    "Normalisation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# From xarray, X has shape: (N_PROF,)\n",
    "# For scikit-learn we need X with shape: [N_PROF,1]\n",
    "X0 = X.values[np.newaxis].T\n",
    "print(\"Data shape [n_samples, n_features]:\", X0.shape) # shape [n_samples, n_features=1]\n",
    "\n",
    "# Fit the scaler object:\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler = scaler.fit(X0)\n",
    "\n",
    "# The mean and std profiles are in the scaler object properties:\n",
    "X_ave = scaler.mean_\n",
    "X_std = scaler.scale_\n",
    "print(\"Data mean, std:\", X_ave, X_std)\n",
    "\n",
    "# Normalize data:\n",
    "Xn = scaler.transform(X0)       \n",
    "\n",
    "# Here, we only center data:\n",
    "Xc = preprocessing.StandardScaler(with_std=False).fit(X0).transform(X0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=plt.figaspect(0.5), dpi=120)\n",
    "axes.set_title('PDF and Gaussian fit to normalised data')\n",
    "ax = sns.histplot(Xn, stat='density', kde=False)\n",
    "plot_normal(np.mean(Xn), np.std(Xn), color=ax.patches[-1].get_facecolor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering in 1D\n",
    "\n",
    "Clearly from the figure above, one can see that the dataset is not Gaussian and exhibits several modes. In other words, data samples agregated into several clusters.\n",
    "\n",
    "Let's identify them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a GMM to identify modes (the \"clusters\") in the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "\n",
    "# Clustering with GMM:\n",
    "classifier = GMM(n_components=4)\n",
    "classifier = classifier.fit(Xn)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels:\n",
    "labels = classifier.predict(Xn)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See more about clusters:\n",
    "n_clusters = np.unique(labels).shape[0]\n",
    "for k in range(n_clusters):\n",
    "    print(\"Cluster %i mean(std), weight: %0.2f (%0.2f), %0.2f\"%(k, \n",
    "                                                                classifier.means_[k,0], \n",
    "                                                                classifier.covariances_[k,0],\n",
    "                                                                classifier.weights_[k]))\n",
    "print(np.sum(classifier.weights_)) # Must be 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the GMM dataset pdf:\n",
    "x = np.linspace(-3,3,200)\n",
    "gmm_pdf = np.zeros(x.shape)\n",
    "for k in range(n_clusters):\n",
    "    gmm_pdf += classifier.weights_[k]*gaussian(x, \n",
    "                                               classifier.means_[k,0], \n",
    "                                               classifier.covariances_[k,0])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.husl_palette(n_clusters)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), dpi=90)\n",
    "ax = sns.histplot(Xn, stat='density', kde=False, ax=ax)\n",
    "ax.set_title('Histogram of normalised data with GMM clustering results (%i clusters)' % n_clusters)\n",
    "plt.plot(x,gmm_pdf,'k', linewidth=2)\n",
    "for k, col in zip(range(n_clusters),colors):\n",
    "    plt.plot(x,classifier.weights_[k]*\n",
    "             gaussian(x, classifier.means_[k,0], classifier.covariances_[k,0]),\\\n",
    "             color=col, linewidth=2, \n",
    "             label=\"$\\lambda_%i=%0.0f$%%: $\\mu_%i$=%0.2f ($\\sigma^2_%i=%0.2f$)\"%(k, \n",
    "                                                    classifier.weights_[k]*100, k, \n",
    "                                                    classifier.means_[k,0], k, \n",
    "                                                    classifier.covariances_[k,0]))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to select the nb of cluster ?\n",
    "\n",
    "This is the most problematic question in clustering.\n",
    "\n",
    "It is a rather difficult problem to determine automatically the most appropriate number of components. There exist different methods that are mostly based on estimating the most probable K, or minimizing a given metric such as the mixture entropy or misfit with the observed PDF (Fraley et al., 1998). A popular method is the Bayesian Information Criterion (BIC, Schwarz, 1978). The BIC is an empirical approach of the model probability computed as:\n",
    "\n",
    "$$BIC(K) = -2\\,\\mathcal{L}(K) + N_f(K)\\,\\log(n) \\label{eq:bic}$$\n",
    "\n",
    "where $\\mathcal{L}(K)$ is the log likelihood of the trained model with $K$ classes, $N_f(K)=K-1+K\\,D+K\\,D\\,(D+1)/2$ is the number of independent parameters to be estimated (the sum of the component weights, Gaussian means and covariance matrix elements in the D-dimensional data space) and $n$ is the number of profiles used to train the model.\n",
    "\n",
    "The BIC is empirical because the first r.h.s. term decreases as the number of classes K increases while the second r.h.s. term is a penalty term that increases with K and thus prevents model overfitting the data. The sum of the two terms is expected to exhibit a minimum for the most appropriate $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmax = 20\n",
    "BIC = np.zeros((kmax))\n",
    "BICf = np.zeros((kmax))\n",
    "\n",
    "print(Xn.shape[0])\n",
    "n = Xn.shape[0] # Nb of samples\n",
    "n = 900\n",
    "n = 2196\n",
    "\n",
    "for k in range(kmax):\n",
    "    print(k, kmax)\n",
    "    this_gmm = GMM(n_components=k+1).fit(Xn)\n",
    "    BIC[k] = this_gmm.bic(Xn)\n",
    "    D = 1 # Nb of dimension\n",
    "    Nf = (k+1)-1 + (k+1)*D + (k+1)*D*(D+1)/2 # Nb of independant parameters to estimate\n",
    "#     print Nf, this_gmm._n_parameters()\n",
    "    BICf[k] = -2*n*this_gmm.score(Xn) + Nf*np.log(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), dpi=90)\n",
    "plt.plot(np.arange(kmax)+1,(BIC-np.mean(BIC))/np.std(BIC),label='Raw BIC')\n",
    "plt.plot(np.arange(kmax)+1,(BICf-np.mean(BICf))/np.std(BICf), label='BIC using only samples of independant observations')\n",
    "plt.ylabel('Normalized BIC')\n",
    "plt.xticks(np.arange(kmax)+1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have more observations than the maximum independant sample size, \n",
    "# we can run several BIC computations\n",
    "kmax = 20\n",
    "Nrun = 30\n",
    "BIC = np.zeros((kmax,Nrun))\n",
    "BICf = np.zeros((kmax,Nrun))\n",
    "\n",
    "print(Xn.shape[0])\n",
    "n = Xn.shape[0] # Nb of samples\n",
    "n = 2196 # Nb of independant samples\n",
    "\n",
    "for run in range(Nrun):\n",
    "    print(run, Nrun)\n",
    "    for k in range(kmax):\n",
    "        ii = np.random.choice(range(X.shape[0]), n, replace=False)\n",
    "        this_gmm = GMM(n_components=k+1).fit(Xn[ii])\n",
    "        BIC[k,run] = this_gmm.bic(Xn[ii])\n",
    "        D = 1 # Nb of dimension\n",
    "        Nf = (k+1)-1 + (k+1)*D + (k+1)*D*(D+1)/2 # Nb of independant parameters to estimate\n",
    "    #     print Nf, this_gmm._n_parameters()\n",
    "        BICf[k,run] = -2*n*this_gmm.score(Xn[ii]) + Nf*np.log(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10,5), dpi=90)\n",
    "BICfmean = np.mean(BICf,axis=1)\n",
    "BICfstd = np.std(BICf,axis=1)\n",
    "normBICfmean = (BICfmean-np.mean(BICfmean))/np.std(BICfmean)\n",
    "plt.plot(np.arange(kmax)+1,BICfmean, \n",
    "         label='BIC using independant observations')\n",
    "plt.plot(np.arange(kmax)+1,BICfmean+BICfstd,color=[0.7]*3,linewidth=0.5)\n",
    "plt.plot(np.arange(kmax)+1,BICfmean-BICfstd,color=[0.7]*3,linewidth=0.5)\n",
    "plt.ylabel('BIC')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.xticks(np.arange(kmax)+1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Change the nb of clusters in the GMM.\n",
    "\n",
    "What happens when K=2 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (py38)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
