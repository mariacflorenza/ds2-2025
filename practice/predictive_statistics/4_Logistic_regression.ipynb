{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSjtYDMpUT7D"
      },
      "source": [
        "### Predictive statistics: class example 4, logistic regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYwTY-M_UT7F"
      },
      "source": [
        "For our final topic of this session, we will implement logistic regression, again using scikit-learn. We will start by looking at some toy data as before. In a second example, we will use the \"digits\" data set that is often used as a standard machine learning example for handwriting recognition to see how logistic regression can also be used to treat this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNSmWl-eUT7F"
      },
      "source": [
        "Let's set up the libraries again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tizNsSX8UT7G"
      },
      "outputs": [],
      "source": [
        "# import numpy to generate toy data, and matplotlib to plot it\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# set up matplotlib to show the figures inline\n",
        "plt.ion()\n",
        "%matplotlib inline\n",
        "# import the logistic regression method from scikit-learn\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5-63p6KUT7G"
      },
      "source": [
        "#### Example 1: toy data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tqa-y73UT7H"
      },
      "source": [
        "We will create some toy data for our first simple example. We will define our variable y to belong to class 0 for x < 6, and to class 1 for x >= 6:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViCvTkFdUT7H"
      },
      "outputs": [],
      "source": [
        "# define x\n",
        "x = np.arange(0,12)\n",
        "# define y, which can take values of 0 or 1 for the two \"output\" classes\n",
        "y = np.zeros(np.shape(x))\n",
        "y[x >= 6] = 1\n",
        "# and let's plot our data:\n",
        "plt.figure()\n",
        "plt.plot(x,y,'ko')\n",
        "plt.grid()\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7yO5QDIUT7H"
      },
      "source": [
        "There are no \"noisy\" points (where x < 6, but y = 1) in this toy data set. We will try changing this afterwards to make the problem more realistic.\n",
        "\n",
        "Let's implement the logistic regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irq5IG96UT7I"
      },
      "outputs": [],
      "source": [
        "# create the logistic regression model using default options\n",
        "model = LogisticRegression()\n",
        "# and fit the model to our data\n",
        "model.fit(x[:,None], y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgLLaI1aUT7I"
      },
      "source": [
        "Now we can get the outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Su-ZvaocUT7I"
      },
      "outputs": [],
      "source": [
        "# the probability that each point belongs to each class:\n",
        "p_pred = model.predict_proba(x[:,None])\n",
        "# the predicted value of y (=> the predicted output class):\n",
        "y_pred = model.predict(x[:,None])\n",
        "# the accuracy of the model (a measure of how many points are\n",
        "# assigned to the wrong output class):\n",
        "score_ = model.score(x[:,None], y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmPbawPhUT7J"
      },
      "source": [
        "Let's look at the values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S598-d6FUT7J"
      },
      "outputs": [],
      "source": [
        "print(p_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVsGa3yZUT7J"
      },
      "source": [
        "Here we have a matrix with shape (2,12) -> we have 2 output classes, and 12 input data points. The matrix gives us the probability that each input data point is in each of the 2 output classes. You can see that the sum for each row = 1 : we have 2 class options, so the total probability must be distributed over these 2 options.\n",
        "\n",
        "```y_pred``` tells us which class was assigned by the model for each of our input data points. The matrix will have the same shape as ```y```:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqVA3RdrUT7J"
      },
      "outputs": [],
      "source": [
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnoqiZ8HUT7J"
      },
      "source": [
        "And finally the score tells us how accurate the model is. The metric used here is: number of correct predictions / number of input data points:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4ouPJSBUT7K"
      },
      "outputs": [],
      "source": [
        "print(score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeFkC1qeUT7K"
      },
      "source": [
        "Our example was very simple, with no anomalous points, and the model has correctly assigned all of the classes, so the score is 1.\n",
        "\n",
        "Let's plot the original data and the predictions to finish:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tU6cIKomUT7K"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(x,y,'ko',label='Input data')\n",
        "plt.plot(x,y_pred,'c.',label='Predicted class')\n",
        "plt.grid()\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeY8YJ0cUT7L"
      },
      "source": [
        "Now let's try repeating the example but with some noisy points:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XXiARryUT7L"
      },
      "outputs": [],
      "source": [
        "# define x\n",
        "x = np.arange(0,12)\n",
        "# define y, which can take values of 0 or 1 for the two \"output\" classes\n",
        "y = np.zeros(np.shape(x))\n",
        "y[x >= 6] = 1\n",
        "# add some noise: we'll choose 3 points at random and change their sign\n",
        "import random\n",
        "i = random.sample(range(0,12),3)\n",
        "print('We will change the class of points: {}'.format(i))\n",
        "for ii in i:\n",
        "    if y[ii] == 0:\n",
        "        y[ii]=1\n",
        "    else:\n",
        "        y[ii]=0\n",
        "\n",
        "# and let's plot our data:\n",
        "plt.figure()\n",
        "plt.plot(x,y,'ko')\n",
        "plt.grid()\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iwTFOvZUT7L"
      },
      "source": [
        "Below, you can reimplement the logistic regression following the example above. Does the regression still find the right cutoff value of x for class 0 / class 1? How does the score change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEIrOSuJUT7M"
      },
      "outputs": [],
      "source": [
        "# enter your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2dv1shMUT7M"
      },
      "source": [
        "Finally, we can easily adapt our example to use multiple predictors. Let's use an example where ```y = 1``` if ```x1 - x2 < 12```. We'll add some noise as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffYHZud5UT7M"
      },
      "outputs": [],
      "source": [
        "# define x1 (predictor 1)\n",
        "x1_1D = np.arange(0,12)\n",
        "# define x2\n",
        "x2_1D = np.arange(-10,2)\n",
        "# make a 2D array:\n",
        "(x1,x2)=np.meshgrid(x1_1D,x2_1D)\n",
        "\n",
        "# define y, which can take values of 0 or 1 for the two \"output\" classes\n",
        "y = np.zeros_like(x1)\n",
        "y[(x1 - x2 < 12)] = 1\n",
        "# add some noise: we'll choose 8 points at random and change their sign\n",
        "import random\n",
        "ix = random.sample(range(0,12),8)\n",
        "iy = random.sample(range(0,12),8)\n",
        "print('We will change the class of points: {}'.format(list(zip(x1_1D[iy],x2_1D[ix]))))\n",
        "for ii in zip(ix,iy):\n",
        "    if y[ii] == 0:\n",
        "        y[ii]=1\n",
        "    else:\n",
        "        y[ii]=0\n",
        "\n",
        "# and plot the data:\n",
        "plt.figure()\n",
        "plt.plot(x1[y==0],x2[y==0],'r^',label='Class 0')\n",
        "plt.plot(x1[y==1],x2[y==1],'bs',label='Class 1')\n",
        "plt.grid()\n",
        "plt.xlabel('x1')\n",
        "plt.ylabel('x2')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXPKGvowUT7M"
      },
      "source": [
        "To perform the logistic regression with multiple inputs, we just need to create an input matrix that contains all of our predictors, as for the linear regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WUwXFYGUT7M"
      },
      "outputs": [],
      "source": [
        "# x1, x2 and y are 2D vectors at the moment, so we will reshape them into 1D arrays,\n",
        "# and then combine x1 and x2 into a single input vector:\n",
        "x1r = np.ravel(x1) #1D vector for x1\n",
        "x2r = np.ravel(x2) #1D vector for x2\n",
        "yr = np.ravel(y) #1D vector for y\n",
        "X = np.vstack((x1r,x2r)).T # X contains x1 and x2\n",
        "\n",
        "# set up the model:\n",
        "model = LogisticRegression()\n",
        "# and fit the model to our data\n",
        "model.fit(X, yr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0vSb9o_UT7M"
      },
      "source": [
        "We can make predictions the same way as before. You can try below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXCt6PrjUT7N"
      },
      "outputs": [],
      "source": [
        "# enter your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLzJ6M3VUT7N"
      },
      "source": [
        "#### Example 2: the digits data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUZHyuWNUT7N"
      },
      "source": [
        "To look at a more complicated example, we can apply this method to the digits data set. This is a set of images of handwritten numbers that have been labelled already. A \"classic\" machine learning test is to try to correctly identify these numbers. We will do this here using logistic regression. The data set can be loaded directly using scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oroO9nxUT7N"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_digits\n",
        "digits = load_digits()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUzQ4QY4UT7O"
      },
      "source": [
        "The data set comprises a series of 8 pixel x 8 pixel images (each represented by a matrix of size (8,8)), and a corresponding label for each image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2HwZwTsUT7O"
      },
      "outputs": [],
      "source": [
        "# print the data set sizes:\n",
        "print('Image data array size: {}'.format(digits.data.shape))\n",
        "print('Class information array size: {}'.format(digits.target.shape))\n",
        "print('Possible class values: {}'.format(np.unique(digits.target)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDgR7JgNUT7P"
      },
      "source": [
        "In the examples with the toy data, we had 2 output classes. Here, we can see that we will need 9. Let's look at an example of the image data that will form our X array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn33M5k6UT7P"
      },
      "outputs": [],
      "source": [
        "# select the first image and reshape it:\n",
        "x0 = np.reshape(digits.data[0,:],(8,8))\n",
        "# select the corresponding class label\n",
        "y0 = digits.target[0]\n",
        "\n",
        "# plot the data\n",
        "plt.figure()\n",
        "plt.imshow(x0,cmap='gray')\n",
        "plt.title('Label = {}'.format(y0))\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ5Ef0IHUT7P"
      },
      "source": [
        "You can see that the input data comprises arrays of (8,8) -> 64 elements, containing integer values between 0 and 16, which represent the shades of grey shown in the image. Our X array thus contains 64 predictor values, compared to the 2 predictors that we looked at in the toy example.\n",
        "\n",
        "In a real-world application, to correctly evaluate the performance of our model, we would want to split our data into a training and test set. Here, we will just fit the model to all of our data to see how the method works, but keep in mind that if we wanted to evaluate the performance of our model, we should keep part of the data separate for this purpose.\n",
        "\n",
        "We'll apply the model as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "knFnT0TNUT7Q"
      },
      "outputs": [],
      "source": [
        "# set up the model:\n",
        "model = LogisticRegression()\n",
        "# and fit the model to our data\n",
        "model.fit(digits.data,digits.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYLDGuotUT7Q"
      },
      "source": [
        "And we can get the expected classes the same way as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICOxUB_GUT7R"
      },
      "outputs": [],
      "source": [
        "# the probability that each point belongs to each class:\n",
        "p_pred = model.predict_proba(digits.data)\n",
        "# the predicted value of y (=> the predicted output class):\n",
        "y_pred = model.predict(digits.data)\n",
        "# the accuracy of the model (a measure of how many points are\n",
        "# assigned to the wrong output class):\n",
        "score_ = model.score(digits.data, digits.target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4sbVxZJUT7R"
      },
      "source": [
        "If we look at the predicted probability array, we will see that we have one value for each class, so 10 values for each input data array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lf6ganFWUT7R"
      },
      "outputs": [],
      "source": [
        "print('The shape of the probability array is: {}'.format(p_pred.shape))\n",
        "print('Probability of the first image being in each of the 10 classes:')\n",
        "for i in range(10):\n",
        "    print('Class {}, P({})={:6.4f}'.format(i,i,p_pred[0,i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBIn8RmsUT7R"
      },
      "source": [
        "If we look at the accuracy of the model, we see that it makes no errors: all images used in fitting the model have been correctly classified:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRNJiENEUT7R"
      },
      "outputs": [],
      "source": [
        "print('Model accuracy: {}'.format(score_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUCDS8SuUT7S"
      },
      "source": [
        "We will get a better idea of the model's ability to accurately classify new data if we split the data into a test and training set. Let's try this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aCuxY3-UT7S"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# we will set aside 25% of the data to test the model,\n",
        "# and will fit the model to 75% of the data.\n",
        "# train_test_split will divide the data up for us:\n",
        "x_train, x_test, y_train, y_test = train_test_split(digits.data,\n",
        "                                    digits.target, test_size=0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2JNZ9a0UT7S"
      },
      "source": [
        "Let's fit the model again, using the training data only:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oH9RO_u9UT7S"
      },
      "outputs": [],
      "source": [
        "# set up the model:\n",
        "model = LogisticRegression()\n",
        "# and fit the model to our data\n",
        "model.fit(x_train,y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ-EGpsiUT7S"
      },
      "source": [
        "If we compare the model accuracy on the test vs the training data, we will see that the model makes some errors when presented with data that was not used for the fitting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVhaYq2DUT7S"
      },
      "outputs": [],
      "source": [
        "print('Accuracy evaluated using training data: {}'.format(model.score(x_train,y_train)))\n",
        "print('Accuracy evaluated using training data: {}'.format(model.score(x_test,y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWfxwRMFUT7T"
      },
      "source": [
        "We can look at an example where the model mis-classifies the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daSErFD4UT7T"
      },
      "outputs": [],
      "source": [
        "# get the model predictions for the test data\n",
        "y_pred = model.predict(x_test)\n",
        "# find points where the prediction is wrong\n",
        "iwrong = np.where(y_pred != y_test)[0]\n",
        "# display the image for the first point that is wrongly classified:\n",
        "plt.figure()\n",
        "idata = np.reshape(x_test[iwrong[0]],(8,8))\n",
        "plt.imshow(idata,cmap='gray')\n",
        "plt.title('Assigned class = {}, true class = {}'.format(y_pred[iwrong[0]],y_test[iwrong[0]]))\n",
        "plt.colorbar();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oCCVVY9UT7T"
      },
      "source": [
        "Finally, we can see what the probability for each class was for this example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlFK0KU1UT7U"
      },
      "outputs": [],
      "source": [
        "# and print out the associated probabilities\n",
        "proba = model.predict_proba(x_test[iwrong[0]][None,:])[0]\n",
        "print('Probability for each class:')\n",
        "for i,p in enumerate(proba):\n",
        "    print('Class: {}, probability: {:5.3f}'.format(i,p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER5qfyPxUT7U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}