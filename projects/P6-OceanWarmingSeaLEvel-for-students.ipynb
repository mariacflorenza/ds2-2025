{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3098442-f61b-4d78-941c-244478cc74ee",
   "metadata": {},
   "source": [
    "# P6 → Ocean thermal and haline change contributions to Sea Level trends\n",
    "\n",
    "## Introduction\n",
    "Sea level increases because of changes in currents (dynamic effect) and because of ocean density changes (steric effect). Hence, Sea surface height increases not only as water is added or transported, but also as water warms and its volume expands. Changes in salt content, or salinity, also affect sea levels. These are known as “thermosteric” and “halosteric” changes. This project is about caracterising and looking at the steric contribution to Sea Level Change.\n",
    "\n",
    "You will need to compute ocean density changes contribution to Sea level rises (thermosteric and halosteric effects) and demonstrate that it is the driver of regional sea level change trends.\n",
    "\n",
    "*Bibliography*:\n",
    "\n",
    "- [Ocean and climate scientific sheet](https://ocean-climate.org/wp-content/uploads/2015/03/sea-level_ScientificItems_BD-3.pdf)\n",
    "- [Overview](https://sealevel.nasa.gov/understanding-sea-level/overview)\n",
    "- [Deep-ocean contribution to sea level and energy budget not detectable over the past decade](https://www.nature.com/articles/nclimate2387)\n",
    "- [Last IPCC report on Sea Level changes](https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_Chapter_09.pdf#page=55)\n",
    "- [IPCC fig 9.12](https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_Chapter_09.pdf#page=237)\n",
    "- [Meyssignac et al, 2012](https://www.sciencedirect.com/science/article/pii/S0264370712000464)\n",
    "\n",
    "*Ideas for your project*\n",
    "\n",
    "Below is a list of what you could do in a timely manner in your project.\n",
    "\n",
    "You shall first compute the Sea Level trend and remove its global average. This local trend will be the pattern to explain.\n",
    "\n",
    "Then you should compute the thermal and haline contribution to steric height. To do so you will use the GSW librairy that will help you compute the absolute salinity, conservative temperature and them the thermal expansion and haline contraction coefficient.\n",
    "\n",
    "You may compare the Sea Level trend from altimetry with the steric height from observations. Be careful to compare similar time period.\n",
    "\n",
    "Using the bibliography, you may consider to compare your results with already published ones, to ensure the accuracy of your computation.\n",
    "\n",
    "The EN4 dataset to work with is a global interpolation of all available ocean in-situ observations. You could further compare CMIP6 projections or historical simulations to the EN4 reference.\n",
    "\n",
    "## Description\n",
    "\n",
    "The Steric contribution to Sea Level Anomaly (SSLA) is computed as the vertical integral of density anomalies:\n",
    "$$SSLA = \\frac{-1}{\\rho_0}\\int_{-H}^{0} \\Delta \\rho \\,dz$$\n",
    "which can be further decomposed into thermosteric (temperature) and halosteric (salinity) contributions:\n",
    "$$SSLA = TSLA + HSLA$$\n",
    "with:\n",
    "$$TSLA = -\\int_{-H}^{0} \\alpha \\Delta T \\,dz$$\n",
    "$$HSLA =  \\int_{-H}^{0} \\beta \\Delta S \\,dz$$\n",
    "\n",
    "where:\n",
    "- $\\Delta \\rho$ the density anomaly, refered to a climatic mean,\n",
    "- $\\Delta T$ the temperature anomaly, refered to a climatic mean,\n",
    "- $\\Delta S$ the salinity anomaly, refered to a climatic mean,\n",
    "- $\\rho_0$ is the reference density $1025 kg/m^{3}$, \n",
    "- $\\alpha$ is the ocean thermal expansion coefficient,\n",
    "- $\\beta$ the haline contraction coefficient,\n",
    "- $H$ is the reference depth, set to 700, 1000 or 2000m.\n",
    "\n",
    "$\\alpha$ and $\\beta$ can be computed with the [GSW library](https://teos-10.github.io/GSW-Python/).\n",
    "\n",
    "Note that you should take of the irregular area and thickness of the grid cells when computing horizontal or vertical averages/integrals ([see the P5 notebook](https://github.com/obidam/ds2-2025/blob/main/projects/P5-OceanWarming-for-students.ipynb)).\n",
    "\n",
    "All ocean variables can be obtained directly from the EN4 dataset. Be careful with units. The Sea Level Change from altimetry data is obtainted from the AVISO dataset, also provided by the class data catalogue.\n",
    "\n",
    "The main problem in this project is to calculate $TSLA$ and $HSLA$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0572878-b32e-44e8-89a1-d6d7e635308f",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf4e7c-b97d-49c9-ab16-60df9628995f",
   "metadata": {},
   "source": [
    "## Load all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef3db9-56aa-4d99-94a8-758e0153db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, urllib, tempfile\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    sys.path.append(tmpdirname)\n",
    "    repo = \"https://raw.githubusercontent.com/obidam/ds2-2025/main/\"\n",
    "    urllib.request.urlretrieve(os.path.join(repo, \"utils.py\"), \n",
    "                               os.path.join(tmpdirname, \"utils.py\"))\n",
    "    from utils import check_up_env\n",
    "    ds2tools = check_up_env(with_tuto=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827d244-930f-4a6e-85bf-d0f898931abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy import stats\n",
    "import dask\n",
    "\n",
    "from intake import open_catalog\n",
    "import gsw\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "create_map = ds2tools.create_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d752f1-fd94-4d8d-9f17-faccc7f7b788",
   "metadata": {},
   "source": [
    "## Connect to the data catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2899acdf-cb93-4649-812d-54ccd64ec7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_url = 'https://raw.githubusercontent.com/obidam/ds2-2025/main/ds2_data_catalog.yml'\n",
    "cat = open_catalog(catalog_url)\n",
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d91369-2c04-4607-a35c-3d6c3a6367f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ocean dataset EN4\n",
    "en4 = cat[\"en4\"].to_dask()\n",
    "print(\"Size of the dataset:\", en4.nbytes/1e9,\"Gb\")\n",
    "en4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c03f55d-b351-42c3-9d1a-9a65bd375143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sea Level Anomalies from altimetry:\n",
    "ssh = cat[\"sea_surface_height\"].to_dask()\n",
    "print(\"Size of the dataset:\", ssh.nbytes/1e9,\"Gb\")\n",
    "ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8689fe-f368-4c5a-ba80-5b5a0cb19745",
   "metadata": {},
   "source": [
    "## Connect to a Coiled Dask cluster\n",
    "\n",
    "For computational expensive operation, you better connect to the Coiled Dask cluster available to the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9608f13-dca1-4949-99d1-afbf8322cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "cluster = coiled.Cluster(name=\"ds2-highcpu-binder\", workspace=\"class-2025\")\n",
    "# cluster = coiled.Cluster(name=\"ds2-highmem-binder\", workspace=\"class-2025\")\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58511a43-5a93-4f80-82cd-0758784d4569",
   "metadata": {},
   "source": [
    "# Sea Level Change from Altimetry\n",
    "Before moving on to computing TSLA and HSLA, you can compute time series and maps of the SSH variable to familiarize with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc1169-eec1-4388-b7e0-2eb2e7eaa15a",
   "metadata": {},
   "source": [
    "### Surface elements for global horizontal averaging\n",
    "To compute a global mean, you need to weight local values with the grid cell areas, in order to account for the latitude dependance of the grid.\n",
    "\n",
    "We'll be using the formula for the area element in spherical coordinates. The [area element for lat-lon coordinates](https://en.wikipedia.org/wiki/Spherical_coordinate_system#Integration_and_differentiation_in_spherical_coordinates) is\n",
    "\n",
    "$$ \\delta A (x,y) = R^2 \\delta \\phi \\delta \\lambda \\cos(\\phi) $$\n",
    "\n",
    "where $\\phi$ is latitude, $\\delta \\phi$ is the spacing of the points in latitude, $\\delta \\lambda$ is the spacing of the points in longitude, and $R$ is Earth's radius. (In this formula, $\\phi$ and $\\lambda$ are measured in radians.) Let's use xarray to create the weight factor.\n",
    "\n",
    "Create a function that compute the surface element $\\delta A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f8e9c-5b2f-43f4-9339-d52243f7a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dA(ds):\n",
    "    # Earth Radius in meters:\n",
    "    R = 6.37e6  \n",
    "    # we know already that the spacing of the points is one degree latitude:\n",
    "    dϕ = np.deg2rad(1.)\n",
    "    dλ = np.deg2rad(1.)\n",
    "    # Apply formulae for area element for lat-lon coordinates:\n",
    "    dA = \n",
    "    return xr.DataArray(dA, dims='latitude', name='dA', attrs={'unit': 'm2', 'long_name': 'Grid surface elements'})\n",
    "\n",
    "ssh['dA'] = get_dA(ssh)\n",
    "ssh['dA'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed294f3d-4847-4aac-98b7-522ac7eba4a3",
   "metadata": {},
   "source": [
    "We can now compute the sea level anomaly time series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64619df8-08c5-49d1-b36c-821f58905f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gsla = ssh['sla'].weighted(ssh['dA']).mean(dim=['latitude', 'longitude'])\n",
    "gsla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56876a4a-282c-4b3c-898b-c89f8833268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsla.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6fc04e-7aa8-4ed8-b911-49e6f7524ec3",
   "metadata": {},
   "source": [
    "### Local Sea Level Trends\n",
    "\n",
    "What we're realy interested in the local trend of SLA.\n",
    "\n",
    "To compute local trends, we will let xarray handle the latitude and longitude dimensions and simply works with xarray DataArrays.\n",
    "\n",
    "Local trends can be computed as the ratio of the time/sla covariance with time variance. \n",
    "\n",
    "So let's define the following sum of squares:\n",
    "\n",
    "for time:\n",
    "$$ss_{tt} = \\Sigma_{i=1}^{N} (t_i - \\bar{t})^2$$\n",
    "\n",
    "for sla:\n",
    "$$ss_{hh} = \\Sigma_{i=1}^{N} (h_i -\\bar{h})^2$$\n",
    "\n",
    "for time/sla:\n",
    "$$ss_{th} = \\Sigma_{i=1}^{N} (t_i - \\bar{t})(h_i -\\bar{h})$$\n",
    "\n",
    "Then, the slope of the linear least square fit is simply given by:\n",
    "$$a = \\frac{ss_{th}}{ss_{tt}}$$\n",
    "\n",
    "If we further estimate the variance for the fit error as:\n",
    "$$s^2 = \\frac{ss_{hh} - a ss_{th}}{N-2}$$\n",
    "\n",
    "the standard error on the slope $a$ is given by:\n",
    "$$SE(a) = \\frac{s}{\\sqrt{ss_{tt}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b361926e-e260-42b3-8e2a-d7c715e46334",
   "metadata": {},
   "source": [
    "Let's now compute all the required sum of squares.\n",
    "\n",
    "Note that for time, we need to convert the numpy datetime format to a more numerical value, like julian days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b920f5-3cc4-4f74-9e7a-324fbf23fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ssh.where(ssh['time']>=pd.to_datetime('19930101'), drop=True).where(ssh['time']<=pd.to_datetime('20100101'), drop=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb3dd39-cebe-41a5-b0ff-9d5fbdedd9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# For time:\n",
    "ds['t'] = xr.DataArray(ds['time'].to_pandas().index.to_julian_date().values, dims='time', name='juld')\n",
    "sstt = ((ds['t']-ds['t'].mean(dim='time'))**2).sum(dim='time')\n",
    "\n",
    "# For sla:\n",
    "sshh = ((ds['sla']-ds['sla'].mean(dim='time'))**2).sum(dim='time').compute().persist()\n",
    "\n",
    "# For time vs sla:\n",
    "ssth = (((ds['t'] - ds['t'].mean(dim='time')) * (ds['sla'] - ds['sla'].mean(dim='time'))).sum(dim='time')).compute().persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ee698-77e4-43a8-8467-020797fe1db2",
   "metadata": {},
   "source": [
    "Let's compute the slope of the linear least square fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b31e22f-e4a1-4e91-95fa-f251abf4ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ssh['sla_trend'] = ssth / sstt  # m / days\n",
    "ssh['sla_trend'].compute().persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4ba91-3559-4346-8bb9-60e734b9130b",
   "metadata": {},
   "source": [
    "And finaly remove the global sea level trend to get only local anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a82972-bce7-46a3-a59a-a3a5b5dbeeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_sla_trend = ssh['sla_trend'].weighted(ssh['dA']).mean()\n",
    "global_sla_trend*365*1000  # mm/year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c89b1f-2026-40dc-92cb-2c2920563363",
   "metadata": {},
   "source": [
    "Let's now make a nice map of the local SLA trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5648d24b-ba46-47a3-a4ce-0e21c2047232",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, proj, ax = create_map()\n",
    "# fig, proj, ax = create_map(extent=[-90, 0, 0, 80])\n",
    "\n",
    "((ssh['sla_trend']-global_sla_trend)*365*1000).plot(transform=proj, ax=ax, \n",
    "                     levels=15, vmin=-14, vmax=14, \n",
    "                     cmap=mpl.colormaps.get_cmap('bwr'),                     \n",
    "                     cbar_kwargs={'shrink': 0.8})\n",
    "ax.add_feature(cfeature.LAND, facecolor=[0.7]*3, zorder=100)\n",
    "ax.set_title(\"Local Sea Level Anomaly trend in mm/year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44200d8e-1531-471e-9215-cce58fc35c2c",
   "metadata": {},
   "source": [
    "# Steric contributions to SLA trends\n",
    "\n",
    "We shall now focus on the core of the P6 project: \n",
    "$$TSLA = -\\int_{-H}^{0} \\alpha \\Delta T \\,dz$$\n",
    "$$HSLA =  \\int_{-H}^{0} \\beta \\Delta S \\,dz$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be16ba2-d5d2-4859-8815-135abf035cba",
   "metadata": {},
   "source": [
    "## Grid elements\n",
    "\n",
    "But we first need the grid elements to compute the vertical and horizontal averages correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6f1db-d612-47e8-a067-0adfd2554859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dA(ds):\n",
    "    # Earth Radius in meters:\n",
    "    R = 6.37e6  \n",
    "    # we know already that the spacing of the points is one degree latitude:\n",
    "    dϕ = np.deg2rad(1.)\n",
    "    dλ = np.deg2rad(1.)\n",
    "    # Apply formulae for area element for lat-lon coordinates:\n",
    "    dA = \n",
    "    return xr.DataArray(dA, dims='lat', name='dA', attrs={'unit': 'm2', 'long_name': 'Grid surface elements'})\n",
    "\n",
    "def get_dz(ds):\n",
    "    depth = ds['depth'].values\n",
    "    dz_0 = depth[0] + (depth[1]-depth[0])/2\n",
    "    dz_i = (depth[1:-1]-depth[:-2])/2 + (depth[2:]-depth[1:-1])/2\n",
    "    dz_N = (depth[-1]-depth[-2])/2\n",
    "    dz = np.concatenate((dz_0[np.newaxis], dz_i, dz_N[np.newaxis]))\n",
    "    return xr.DataArray(dz, dims='depth', name='dZ', attrs={'unit': 'm', 'long_name': 'Grid vertical thickness'})\n",
    "\n",
    "en4['dA'] = get_dA(en4);\n",
    "en4['dZ'] = get_dz(en4);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e5c39-2c34-4d48-b27d-fa8ce4994b09",
   "metadata": {},
   "source": [
    "## Thermal and haline coefficients\n",
    "\n",
    "Now let's use the GSW library to compute the absolute salinity and conservative temperature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64421ea0-e933-4965-84c4-bcbd1dc1be94",
   "metadata": {},
   "outputs": [],
   "source": [
    "en4['pres'] = gsw.p_from_z(-en4['depth'], en4['lat'])\n",
    "en4['pres'].compute().persist()\n",
    "\n",
    "en4['SA'] = gsw.SA_from_SP(en4['salinity'], en4['pres'], en4['lon'], en4['lat'])\n",
    "en4['SA'].attrs = {'unit': 'g/kg', 'long_name': 'Absolute Salinity'}\n",
    "\n",
    "en4['CT'] = gsw.CT_from_t(en4['SA'], (en4['temperature']-273.15), en4['pres'])\n",
    "en4['CT'].attrs = {'unit': 'degC', 'long_name': 'Conservative Temperature of seawater from in-situ temperature'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb1d69-8cf6-4a99-81a9-0c4df7051f5b",
   "metadata": {},
   "source": [
    "and finaly we can compute the $\\alpha$ and $\\beta$ coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8a17b0-c0c0-47bc-b95e-c6c373d10889",
   "metadata": {},
   "outputs": [],
   "source": [
    "en4['alpha'] = \n",
    "en4['alpha'].attrs = {'unit': '1/K', 'long_name': 'Thermal expansion coefficient'}\n",
    "\n",
    "en4['beta'] = \n",
    "en4['beta'].attrs = {'unit': 'kg/g', 'long_name': 'Haline contraction coefficient'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c9045-361e-4d60-a971-5b8901d0e3a1",
   "metadata": {},
   "source": [
    "Let's have a look at those coefficients\n",
    "\n",
    "Check should compare to:\n",
    "\n",
    "https://www.science.org/doi/10.1126/sciadv.abq0793#:~:text=In%20the%20global%20ocean%2C%20the,C%E2%88%921%20in%20tropical%20waters.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Haline_contraction_coefficient#/media/File:Beta_30W.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bb059-97b2-4869-b841-f16249d17919",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "da = en4['alpha'].isel(depth=0).weighted(en4['dA']).mean(dim=['time', 'lon'])\n",
    "da.attrs = en4['alpha'].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cdcc3c-df0f-4460-b60a-d47670f7b865",
   "metadata": {},
   "outputs": [],
   "source": [
    "(da/1e-4).plot(y='lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f4a0a2-32c5-404e-8fc0-0c332fb2bbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "da = en4['beta'].sel(lon=360-30, method='nearest').weighted(en4['dA']).mean(dim=['time']).compute().persist()\n",
    "da.attrs = en4['beta'].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed46194-daa9-432e-8464-2993184c6eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.plot(yincrease=False, levels=12, figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948ff91-914d-4048-a963-6679a34864f7",
   "metadata": {},
   "source": [
    "## Steric heights\n",
    "\n",
    "In order to compute the steric height contribution from changes in temperature or salinity, we need a reference level for the integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e9c48-6e9a-4e30-9731-bf94ce6219dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "H =   # in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb165e95-0124-4d62-8b8c-4ed3e6dec98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "en4['DT'] = en4['temperature'] - en4['temperature'].mean(dim='time')\n",
    "en4['TSLA'] = \n",
    "en4['TSLA'] = en4['TSLA'].compute().persist()\n",
    "en4['TSLA'].attrs = {'unit': 'm', 'long_name': 'Thermosteric height'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e214f-3ef6-416e-a4df-a4e70a06e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "en4['DS'] = en4['SA'] - en4['SA'].mean(dim='time')\n",
    "en4['HSLA'] = \n",
    "en4['HSLA'] = en4['HSLA'].compute().persist()\n",
    "en4['HSLA'].attrs = {'unit': 'm', 'long_name': 'Halosteric height'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9640f4f-ac18-4c00-8198-4fd817902d6c",
   "metadata": {},
   "source": [
    "## Local trends of steric height components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712c4196-2e93-4cae-8d15-743b55f57ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trend(ds, vname, time='time'):\n",
    "    # For time:\n",
    "    ds['t'] = xr.DataArray(ds[time].to_pandas().index.to_julian_date().values, dims=time, name='juld')\n",
    "    sstt = ((ds['t']-ds['t'].mean(dim=time))**2).sum(dim=time)\n",
    "\n",
    "    # For vname:\n",
    "    ssyy = ((ds[vname]-ds[vname].mean(dim=time))**2).sum(dim=time)\n",
    "\n",
    "    # For time vs vname:\n",
    "    ssty = (((ds['t'] - ds['t'].mean(dim=time)) * (ds[vname] - ds[vname].mean(dim=time))).sum(dim=time))\n",
    "\n",
    "    # Trend:\n",
    "    trend = ssty / sstt  # [unit] / days\n",
    "    \n",
    "    return trend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d3b4aa-44f4-45e7-856a-78f5de9d0959",
   "metadata": {},
   "source": [
    "Let's define the time period to look at:\n",
    "\n",
    "1993-2010 for instance, to compare with results from [Meyssignac et al, 2012](https://www.sciencedirect.com/science/article/pii/S0264370712000464)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360bbc0d-473a-4f37-a8cf-02562a3d015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = en4.where(en4['time']>=ssh['time'].min(), drop=True).where(en4['time']<=ssh['time'].max(), drop=True)\n",
    "ds = en4.where(en4['time']>=pd.to_datetime('19930101'), drop=True).where(en4['time']<pd.to_datetime('20110101'), drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddd02fe-9379-416f-a5c4-9426ae40f1bd",
   "metadata": {},
   "source": [
    "and compute trends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eedcda-b9a1-4b83-a1b5-bc97c7415c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds['HSLA_trend'] = compute_trend(ds, 'HSLA').compute().persist()\n",
    "ds['TSLA_trend'] = compute_trend(ds, 'TSLA').compute().persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb6242c-8362-4233-96a5-d8dc7341abd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_steric_trend = (ds['TSLA_trend']+ds['HSLA_trend']).weighted(ds['dA'].fillna(0)).mean()\n",
    "global_steric_trend*365*1000  # mm/year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f86b7d-958c-404d-bfe6-fadb94b0b8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, proj, ax = create_map()\n",
    "((ds['TSLA_trend']+ds['HSLA_trend']-global_steric_trend)*365*1000).plot(transform=proj, ax=ax,   \n",
    "                     levels=15, vmin=-14, vmax=14, \n",
    "                     cmap=mpl.colormaps.get_cmap('bwr'))\n",
    "ax.add_feature(cfeature.LAND, facecolor=[0.7]*3, zorder=100)\n",
    "ax.set_title(\"Local Steric Sea Level Anomaly trend in mm/year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1c325e-ba54-487a-bd92-c3ac9d226e5d",
   "metadata": {},
   "source": [
    "## Compare steric trend to altimetry\n",
    "\n",
    "You shall now interpolate all sea level trends and compare them on maps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd14b3a9-b411-4bc3-aecc-ae857c003c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, proj, ax = create_map()\n",
    "# fig, proj, ax = create_map(extent=[-90, 0, 0, 80])\n",
    "\n",
    "((ssh['sla_trend']-global_sla_trend)*365*1000).plot(transform=proj, ax=ax, \n",
    "                     levels=15, vmin=-14, vmax=14, \n",
    "                     cmap=mpl.colormaps.get_cmap('bwr'),                     \n",
    "                     cbar_kwargs={'shrink': 0.8})\n",
    "ax.add_feature(cfeature.LAND, facecolor=[0.7]*3, zorder=100)\n",
    "ax.set_title(\"Local Sea Level Anomaly trend in mm/year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045e3ad9-d8cd-449c-b366-b192bc144b13",
   "metadata": {},
   "source": [
    "## Timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ebffa0-c355-4315-a546-07d1905b7c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2-coiled-2025-binder",
   "language": "python",
   "name": "ds2-coiled-2025-binder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
