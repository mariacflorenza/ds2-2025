{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3098442-f61b-4d78-941c-244478cc74ee",
   "metadata": {},
   "source": [
    "# P5 → Ocean Warming \n",
    "\n",
    "## Introduction\n",
    "Because of the human driven intensification of the greenhouse effect, the ocean is warming. This project is about caracterising and looking at this warming. \n",
    "\n",
    "*Bibliography*:\n",
    "\n",
    "- [Ocean Climate scientific sheet](https://ocean-climate.org/wp-content/uploads/2020/01/1.-The-ocean-a-heat-reservoir-scientific-fact-sheets-2019.pdf)\n",
    "- [Last IPCC report on Ocean Heat Content changes](https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_Chapter_09.pdf#page=35)\n",
    "- [IPCC fig 9.6](https://www.ipcc.ch/report/ar6/wg1/downloads/report/IPCC_AR6_WGI_Chapter_09.pdf#page=227)\n",
    "- [IUCN brief on Ocean warming](https://www.iucn.org/resources/issues-briefs/ocean-warming)\n",
    "- [Global Ocean Heat and Salt Content: Seasonal, Yearly, and Pentadal Fields](https://www.ncei.noaa.gov/access/global-ocean-heat-content/)\n",
    "- [European Indicator](https://marine.copernicus.eu/access-data/ocean-monitoring-indicators/global-ocean-heat-content-0-2000m)\n",
    "\n",
    "*Ideas for your project*\n",
    "\n",
    "Below is a list of what you could do in a timely manner in your project.\n",
    "\n",
    "You shall first compute ocean heat content (OHC) and caracterise its timeseries. Then you could compute its trend with a regression (linear or not) for the entire ocean time series and extrapolate to the future, e.g. what is the expected ocean warming for the horizon 2100?\n",
    "\n",
    "Instead of working globally, you could also study the ocean warming locally. In this case, you can plot the local slopes of the different OHC, or temperature time series and deduce where the ocean warming is moderate and where it is strong. You could answer the question of where global warming affects most the ocean ?\n",
    "\n",
    "The EN4 dataset to work with is a global interpolation of all available ocean in-situ observations. You could further compare CMIP6 projections or historical simulations to the EN4 reference.\n",
    "\n",
    "## Description\n",
    "Ocean Heat Content $OHC(x, y, z, t)$ is expressed in Joules an is given by:\n",
    "$$OHC(x,y,z,t) = \\rho \\cdot C_p \\cdot T(x,y,z,t) \\cdot \\delta V(x,y,z)$$\n",
    "\n",
    "where $\\rho$ is density and $C_p$ heat capacity:\n",
    "* $\\rho$ = 1035 $kg/m^{3}$\n",
    "* $C_{\\rho}$ =  4,186 J/(kg ⋅ K)\n",
    "  \n",
    "They will be considered as constant for the project.\n",
    "\n",
    "The ocean temperature $T$ can be obtained directly from the dataset. Be careful with units.\n",
    "\n",
    "The main problem in this project is to calculate the volume element $\\delta V$ for a given latitude, longitude and depth. This volume element represents the ocean volume where temperature $T(x,y,z,t)$ is considered constant in the dataset. \n",
    "The larger the volume, the larger the error compared to the real ocean. So, we'll keep in mind that the dataset, although based on observations, remains an interpolation with a limited scientific scope, given by the grid resolution.\n",
    "\n",
    "The P5 project requires you to compute the $OHC(x,y,z,t)$ variable and to explore its properties, e.g.: 0-700m vertical integral, zonal integral, global timeseries, 0-700 mean temperature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0572878-b32e-44e8-89a1-d6d7e635308f",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcf4e7c-b97d-49c9-ab16-60df9628995f",
   "metadata": {},
   "source": [
    "## Load all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebffea1-7526-4faa-851b-0b149e345784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, urllib, tempfile\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    sys.path.append(tmpdirname)\n",
    "    repo = \"https://raw.githubusercontent.com/obidam/ds2-2025/main/\"\n",
    "    urllib.request.urlretrieve(os.path.join(repo, \"utils.py\"), \n",
    "                               os.path.join(tmpdirname, \"utils.py\"))\n",
    "    from utils import check_up_env\n",
    "    ds2tools = check_up_env(with_tuto=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6827d244-930f-4a6e-85bf-d0f898931abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy import stats\n",
    "import dask\n",
    "\n",
    "from intake import open_catalog\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "create_map = ds2tools.create_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d752f1-fd94-4d8d-9f17-faccc7f7b788",
   "metadata": {},
   "source": [
    "## Connect to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e99ac7-32f0-466f-b003-21c98b16a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog_url = 'https://raw.githubusercontent.com/obidam/ds2-2025/main/ds2_data_catalog.yml'\n",
    "cat = open_catalog(catalog_url)\n",
    "ds = cat[\"en4\"].to_dask()\n",
    "print(\"Size of the dataset:\", ds.nbytes/1e9,\"Gb\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58511a43-5a93-4f80-82cd-0758784d4569",
   "metadata": {},
   "source": [
    "## Explore the dataset\n",
    "Before moving on to computing OHC, you can make some maps of the dataset variables to familiarize with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc6fcf-7c34-4562-8cba-a80b07fcfb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset temperature variable in deg C:\n",
    "T = ds['temperature'] - 273.15  # Since unit is K\n",
    "\n",
    "# Then select the level the closest to the surface, to get an estimate of the Sea Surface Temperarure (SST)\n",
    "SST = T.sel(depth=0, method='nearest')\n",
    "SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e18731-ccb9-4c7f-a01a-b55681892b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rough plot with xarray:\n",
    "SST.isel(time=0).plot(vmin=-2, vmax=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c1390-9ceb-4571-97e5-9a4b2e0dc0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A nicer plot for presentation:\n",
    "fig, proj, ax = create_map()\n",
    "SST.isel(time=0).plot(transform=proj, ax=ax, vmin=-2, vmax=30, cbar_kwargs={'shrink': 0.4})\n",
    "ax.add_feature(cfeature.LAND, facecolor=[0.7]*3, zorder=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d222aeac-ae19-484f-ac15-b23ab11e1988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also use multi-dimensional indexing:\n",
    "T.sel(time='2005-11', method='nearest').sel(depth=1000, method='nearest').plot(vmin=-2, vmax=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8df220-aca8-4072-a7bb-ab1f3205f217",
   "metadata": {},
   "source": [
    "# Analyse OHC and temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d252139-a8fc-41b7-a3ab-028864734501",
   "metadata": {},
   "source": [
    "## Compute volume elements\n",
    "With the EN4 dataset, the grid is a regular Mercator projection, i.e. it has constant spacing in latitude and longitude in degrees. In this case, each grid point is separated by 1 degree from nearby grid points.\n",
    "\n",
    "To account for the spherical geometry of the globe, the volume element $\\delta V$ will smaller near the pole than at the equator. Since the EN4 dataset grid is regular, volume elements only depends on latitude and depth (because the vertical depth axis is not regular).\n",
    "\n",
    "We will decompose it as:\n",
    "$$\\delta V (y,z) = \\delta A(y) \\cdot \\delta z(z)$$\n",
    "\n",
    "Although xarray is very useful for geospatial analysis, **it has no built-in understanding of geography**.\n",
    "\n",
    "Below we show how to create variables to determine the volume element $\\delta V$ and be able to compute a proper weighted sum for OHC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cc1169-eec1-4388-b7e0-2eb2e7eaa15a",
   "metadata": {},
   "source": [
    "### Surface elements\n",
    "We'll be using the formula for the area element in spherical coordinates. The [area element for lat-lon coordinates](https://en.wikipedia.org/wiki/Spherical_coordinate_system#Integration_and_differentiation_in_spherical_coordinates) is\n",
    "\n",
    "$$ \\delta A (x,y) = R^2 \\delta \\phi \\delta \\lambda \\cos(\\phi) $$\n",
    "\n",
    "where $\\phi$ is latitude, $\\delta \\phi$ is the spacing of the points in latitude, $\\delta \\lambda$ is the spacing of the points in longitude, and $R$ is Earth's radius. (In this formula, $\\phi$ and $\\lambda$ are measured in radians.) Let's use xarray to create the weight factor.\n",
    "\n",
    "Create a function that compute the surface element $\\delta A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f8e9c-5b2f-43f4-9339-d52243f7a6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dA(ds):\n",
    "    # Earth Radius in meters:\n",
    "    R = 6.37e6  \n",
    "    # we know already that the spacing of the points is one degree latitude:\n",
    "    dϕ = np.deg2rad(1.)\n",
    "    dλ = np.deg2rad(1.)\n",
    "    # Apply formulae for area element for lat-lon coordinates:\n",
    "    dA = \n",
    "    return xr.DataArray(-dA, dims='lat', name='dA', attrs={'unit': 'm2', 'long_name': 'Grid surface elements'})\n",
    "\n",
    "ds['dA'] = get_dA(ds)\n",
    "ds['dA'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6c0d1-a9bd-41fb-bce6-dc8aefe6614b",
   "metadata": {},
   "source": [
    "### Depth elements\n",
    "\n",
    "The vertical axis is trickier because depth levels are not regular. By considering the given depth as the center of cubes, you can divide the cubes in the middle of 2 given depths. For the first and the last cube, you can start from the surface of the ocean and end at the last depth available.\n",
    "\n",
    "The following plot illustrate the thickness of the vertical grid depending on depth levels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea868099-a273-42fc-b2d4-6b818da16f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['depth'].diff(dim='depth').plot(y='depth', yincrease=False, marker='.')\n",
    "plt.xlabel(\"Depth cell thickness [m]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea5b9d-231f-4149-be86-b6ccbfb3125c",
   "metadata": {},
   "source": [
    "Write a function that computes depth thickness elements $\\delta z$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dea397-e923-4d3b-b50e-4a1b1cc97acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dz(ds):\n",
    "    depth = ds['depth'].values\n",
    "    dz_0 = depth[0] + (depth[1]-depth[0])/2\n",
    "    dz_i = (depth[1:-1]-depth[:-2])/2 + (depth[2:]-depth[1:-1])/2\n",
    "    dz_N = (depth[-1]-depth[-2])/2\n",
    "    dz = np.concatenate((dz_0[np.newaxis], dz_i, dz_N[np.newaxis]))\n",
    "    return xr.DataArray(dz, dims='depth', name='dZ', attrs={'unit': 'm', 'long_name': 'Grid vertical thickness'})\n",
    "    \n",
    "ds['dZ'] = get_dz(ds)\n",
    "ds['dZ'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4fa0ad-ed38-4024-86f5-f1ae27854c71",
   "metadata": {},
   "source": [
    "### Volume elements\n",
    "\n",
    "Now that you have thickness and surface elements, you can simply compute volume elements like this:\n",
    "\n",
    "(xarray will automatically handle the dimensions broadcasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2459873-3d2a-490b-b394-7d9664ec5c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['dV'] = ds['dA'] * ds['dZ']\n",
    "ds['dV'].attrs = {'unit': 'm3', 'long_name': 'Grid volume elements'}\n",
    "ds['dV']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e787e-48ed-499a-881d-a5a1b1b17f73",
   "metadata": {},
   "source": [
    "## Timeseries of OHC\n",
    "\n",
    "### Connect to a Coiled Dask cluster\n",
    "\n",
    "But this will be a computational expensive operation, so you better connect to the Coiled Dask cluster available to the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9608f13-dca1-4949-99d1-afbf8322cea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "cluster = coiled.Cluster(name=\"ds2-highcpu-binder\", workspace=\"class-2025\")\n",
    "# cluster = coiled.Cluster(name=\"ds2-highmem-binder\", workspace=\"class-2025\")\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32506451-73f7-4cf6-9142-2f0c84332053",
   "metadata": {},
   "source": [
    "### Compute OHC\n",
    "\n",
    "You now have everything you need to compute the 4-dimensional OHC variable and check its time evolution.\n",
    "\n",
    "We recall that the Ocean Heat Content $OHC(x, y, z, t)$ is expressed in Joules an is given by:\n",
    "$$OHC(x,y,z,t) = \\rho \\cdot C_p \\cdot T \\cdot \\delta V$$\n",
    "\n",
    "Again, you can let xarray to handle the dimensions broadcasting for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fcf3eb-be0c-406f-809f-0a2d8bf38c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "rho = 1035 # kg/m3\n",
    "Cp = -4186 # J/K/kg\n",
    "\n",
    "ds['OHC'] = \n",
    "ds['OHC'].attrs = {'unit': 'J', 'long_name': 'Ocean Heat Content'}\n",
    "ds['OHC']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c83f5-6ddc-4d40-bb01-6223d3a3c804",
   "metadata": {},
   "source": [
    "### Compute timeseries for a specific layer\n",
    "\n",
    "A classic way to visualise these timeseries is by integrating vertically only between the surface and 700m. This ensures a more reliable and consistent estimate throughout the time period, starting in 1950.\n",
    "\n",
    "Moreover, global OHC timeseries are often given in $ZJ = 10^{21} J$ and as anomaly compared to the first timeseries value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865fb76-1dd8-47b3-8fbd-68f64c60dc4b",
   "metadata": {},
   "source": [
    "Let's now select the ocean layer 0-700 and compute the heat content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d404bb09-37db-493a-8752-b76ac6913608",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ohc700 = \n",
    "ohc700"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042bf072-b691-4eaf-b16f-a7a645b4277c",
   "metadata": {},
   "source": [
    "It is important to note here that we did not yet computed anything, only the dimensions and coordinates where determined by xarray.\n",
    "\n",
    "IT IS ONLY WHEN ARRAY VALUES ARE NEEDED THAT THE COMPUTATION REALY HAPPENS !\n",
    "\n",
    "This is the case when doing a plot for instance. So to avoid to recompute `ohc700` every time we're plotting it, you can manually trigger the computation and make the result persistant like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd79094-7d1d-4d6e-8d97-76bb0033035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohc700 = ohc700.compute().persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5d58f-673e-4fb3-8f63-88bffa87e64f",
   "metadata": {},
   "source": [
    "And finaly we can make the OHC 0-700m timeseries plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eeaee2-585e-493a-9948-b20914374cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = ohc700.isel(time=0)\n",
    "unit = 1e21 # Zeta-joules\n",
    "\n",
    "((ohc700 - origin)/unit).plot()\n",
    "plt.title(\"OHC 0-700m timeseries from the EN4 dataset\\n(in ZJ=10^{21}J and anomaly wrt %s)\" % pd.to_datetime(origin['time'].data))\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe292c1-15c9-4db8-8764-fec05d6454e8",
   "metadata": {},
   "source": [
    "## Local temperature trends\n",
    "\n",
    "It is very interesting to be able to visualise a map of the local temperature trends in order to understand where is ocean warming the strongest.\n",
    "\n",
    "To compute local trends, we will let xarray handle the latitude and longitude dimensions and simply works with xarray DataArrays.\n",
    "\n",
    "Local trends can be computed as the ratio of the time/temperature covariance with time variance. \n",
    "\n",
    "So let's define the following sum of squares:\n",
    "\n",
    "for time:\n",
    "$$ss_{tt} = \\Sigma_{i=1}^{N} (t_i - \\bar{t})^2$$\n",
    "\n",
    "for temperature:\n",
    "$$ss_{\\theta\\theta} = \\Sigma_{i=1}^{N} (\\theta_i -\\bar{\\theta})^2$$\n",
    "\n",
    "for time/temperature:\n",
    "$$ss_{t\\theta} = \\Sigma_{i=1}^{N} (t_i - \\bar{t})(\\theta_i -\\bar{\\theta})$$\n",
    "\n",
    "Then, the slope of the linear least square fit is simply given by:\n",
    "$$a = \\frac{ss_{t\\theta}}{ss_tt}$$\n",
    "\n",
    "If we further estimate the variance for the fit error as:\n",
    "$$s^2 = \\frac{ss_{\\theta\\theta} - a ss_{t\\theta}}{N-2}$$\n",
    "\n",
    "the standard error on the slope $a$ is given by:\n",
    "$$SE(a) = \\frac{s}{\\sqrt{ss_{tt}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1540606-78b8-4605-a44b-8cfe9ddc81cd",
   "metadata": {},
   "source": [
    "So, let's look at the latitude/longitude map of the 0-700 layer mean temperature trend.\n",
    "\n",
    "To compute the 0-700m mean temperature, we need to weight the temperature variable by the grid thickness and to do so we can use the xarray `weighted` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c0be9-2a05-4a1b-8e7d-d8ac0362f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "da = (ds['temperature'].where(ds['depth']<=700).weighted(ds['dZ'])).mean(dim=['depth'])\n",
    "da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b361926e-e260-42b3-8e2a-d7c715e46334",
   "metadata": {},
   "source": [
    "Let's now compute all the required sum of squares.\n",
    "\n",
    "Note that for time, we need to convert the numpy datetime format to a more numerical value, like julian days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a73f670-3ec7-4e45-8aaa-e9bea0a0f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For time:\n",
    "ds['t'] = xr.DataArray(da['time'].to_pandas().index.to_julian_date().values, dims='time', name='juld')\n",
    "ssxx = ((ds['t']-ds['t'].mean(dim='time'))**2).sum(dim='time')\n",
    "\n",
    "# For temperature:\n",
    "ssyy = \n",
    "\n",
    "# For time vs temperature:\n",
    "ssxy = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424ee698-77e4-43a8-8467-020797fe1db2",
   "metadata": {},
   "source": [
    "Finaly compute the slope of the linear least square fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b31e22f-e4a1-4e91-95fa-f251abf4ef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend = ssxy / ssxx  # K / days\n",
    "trend.compute().persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c89b1f-2026-40dc-92cb-2c2920563363",
   "metadata": {},
   "source": [
    "Let's now make a nice map of the temperature trend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5648d24b-ba46-47a3-a4ce-0e21c2047232",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, proj, ax = create_map()\n",
    "# fig, proj, ax = create_map(extent=[-90, 0, 0, 80])\n",
    "\n",
    "(trend*1e3*10).plot(transform=proj, ax=ax, levels=13, cbar_kwargs={'shrink': 0.8})\n",
    "ax.add_feature(cfeature.LAND, facecolor=[0.7]*3, zorder=100)\n",
    "ax.set_title(\"Local 0-700m temperature trend in mK/decade\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f276904-6ea2-4cb2-b7a4-d356152b60ac",
   "metadata": {},
   "source": [
    "## Vertical structure of OHC\n",
    "\n",
    "You have briefly looked at the timeseries and horizontal distribution of OHC and temperature.\n",
    "\n",
    "You can further explore the other dimensions of the dataset.\n",
    "\n",
    "In particular, you may want to look at zonal section across the Pacific or Atlantic oceans, or simply zonal means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6848ef03-ea28-46c8-b847-b56bd858acd6",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "As it says, you may want to try to predict the evolution of OHC700, localy with T700"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c0b2a-a9e1-4cad-8edc-b8888cd5e671",
   "metadata": {},
   "source": [
    "## CMIP6 dataset\n",
    "\n",
    "Since you have access to CMIP6 simulations, you could also look at the OHC700/T700 evolution in climate simulations, trying to compute the projected values in 2100, or to isolate the human finger print of the historical period (piControl vs historical runs).\n",
    "\n",
    "[More on climate model simulations data](https://www.wcrp-climate.org/wgcm-cmip/wgcm-cmip6)\n",
    "\n",
    "[You can check at the CMIP6 practice notebook](https://github.com/obidam/ds2-2025/blob/main/practice/environment/Access-CMIP6-data.ipynb)\n",
    "\n",
    "[Access to CMIP6 data](https://cloud.google.com/blog/products/data-analytics/new-climate-model-data-now-google-public-datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2399e4b-b9fa-41a2-ac4c-4baf71a73206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2-coiled-2025-binder",
   "language": "python",
   "name": "ds2-coiled-2025-binder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
